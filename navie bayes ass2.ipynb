{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf6e34c-9df2-4404-9b24-eabd7b2fd83e",
   "metadata": {},
   "source": [
    "#Q1.\n",
    "\n",
    "To find the probability that an employee is a smoker given that they use the health insurance plan, you can use conditional probability. You are given:\n",
    "\n",
    "    The probability that an employee uses the health insurance plan (P(Insurance)) is 70% or 0.7.\n",
    "    The probability that an employee who uses the health insurance plan is a smoker (P(Smoker|Insurance)) is 40% or 0.4.\n",
    "\n",
    "You want to find P(Smoker|Insurance), which is the probability that an employee is a smoker given that they use the health insurance plan. You can use the formula for conditional probability:\n",
    "\n",
    "P(Smoker∣Insurance)=P(Smoker and Insurance)P(Insurance)P(Smoker∣Insurance)=P(Insurance)P(Smoker and Insurance)​\n",
    "\n",
    "You already have P(Insurance) and P(Smoker|Insurance), so you can calculate P(Smoker and Insurance) as follows:\n",
    "\n",
    "P(SmokerandInsurance)=P(Smoker∣Insurance)⋅P(Insurance)=0.4⋅0.7=0.28P(SmokerandInsurance)=P(Smoker∣Insurance)⋅P(Insurance)=0.4⋅0.7=0.28\n",
    "\n",
    "Now, you can use this information to find P(Smoker|Insurance):\n",
    "\n",
    "P(Smoker∣Insurance)=0.280.7=410=0.4P(Smoker∣Insurance)=0.70.28​=104​=0.4\n",
    "\n",
    "So, the probability that an employee is a smoker given that they use the health insurance plan is 0.4 or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba8e82-b79e-4fce-96b7-2f85b6807f44",
   "metadata": {},
   "source": [
    "#Q2.\n",
    "\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two different variants of the Naive Bayes classifier, and the key difference between them lies in the type of data they are best suited for and how they model the data. Here are the main differences:\n",
    "\n",
    "1. Data Representation:\n",
    "\n",
    "    Bernoulli Naive Bayes: This classifier is designed for binary or Boolean data, where each feature can be either present (1) or absent (0). It is commonly used in text classification tasks where the features represent the presence or absence of words in a document (a \"bag of words\" model).\n",
    "\n",
    "    Multinomial Naive Bayes: This classifier is suitable for data that involves counts or frequencies of discrete features, typically in the form of integer values. It is often used for text classification tasks where the features represent the counts of words or term frequencies.\n",
    "\n",
    "2. Modeling Approach:\n",
    "\n",
    "    Bernoulli Naive Bayes: In Bernoulli Naive Bayes, the model assumes that each feature is a binary variable and is conditionally independent given the class label. It models the presence (1) or absence (0) of each feature in the data.\n",
    "\n",
    "    Multinomial Naive Bayes: Multinomial Naive Bayes is used when dealing with discrete data, and it models the likelihood of observing a specific count of each feature within the class. It assumes that features are generated from a multinomial distribution.\n",
    "\n",
    "3. Feature Independence:\n",
    "\n",
    "    Bernoulli Naive Bayes: Assumes that features are binary and independent given the class. It doesn't consider the number of occurrences, only their presence or absence.\n",
    "\n",
    "    Multinomial Naive Bayes: Assumes that features are discrete and independent given the class but considers the frequency or count of each feature within the class.\n",
    "\n",
    "4. Application:\n",
    "\n",
    "    Bernoulli Naive Bayes: Commonly used for text classification problems, especially sentiment analysis and spam email detection, where the focus is on the presence or absence of words.\n",
    "\n",
    "    Multinomial Naive Bayes: Widely used for text classification tasks, document categorization, and other tasks where features are counts or frequencies, such as text document analysis.\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the data you are working with. If your data is binary, representing the presence or absence of features, Bernoulli Naive Bayes is more appropriate. If your data consists of discrete feature counts or frequencies, Multinomial Naive Bayes is a better choice. Both variants are valuable tools in text classification and other applications where Naive Bayes methods are suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07997519-d5fd-49c3-899e-05242a85f70f",
   "metadata": {},
   "source": [
    "#Q3.\n",
    "\n",
    "Bernoulli Naive Bayes is a classification algorithm designed for binary data, where features are represented as either present (1) or absent (0). When dealing with missing values in the context of Bernoulli Naive Bayes, you typically need to make a decision about how to handle them.\n",
    "\n",
    "Here are a few common approaches to handling missing values when using Bernoulli Naive Bayes:\n",
    "\n",
    "    Imputation with a Default Value: One straightforward approach is to impute the missing values with a default value, such as 0 (absence) or 1 (presence), depending on the context. This effectively assumes that the missing feature is either entirely absent or always present, which may or may not be appropriate, depending on your data.\n",
    "\n",
    "    Imputation with the Mode: Another option is to impute the missing values with the mode (the most frequent value) of that feature. This approach assumes that the feature's most common state is the most likely one when the data is missing. This can be more appropriate if the data distribution suggests a strong bias toward one of the states.\n",
    "\n",
    "    Feature Engineering: You might choose to create a new binary feature that explicitly represents the presence or absence of missing values for each original feature. This can help the model capture the information about missing values explicitly.\n",
    "\n",
    "    Discard Instances with Missing Values: In some cases, if the number of instances with missing values is relatively small and can be safely removed without significantly affecting the dataset, you can consider removing instances with missing values.\n",
    "\n",
    "    Use Advanced Imputation Techniques: Depending on the specific characteristics of your data and the problem you're solving, you may consider more advanced imputation techniques, such as using machine learning algorithms for imputation (e.g., using a separate classifier to predict missing values) or leveraging domain-specific knowledge to handle missing data appropriately.\n",
    "\n",
    "The choice of how to handle missing values in Bernoulli Naive Bayes should be made based on a careful assessment of your dataset and the specific problem you're trying to solve. It's important to consider the implications of the chosen approach on the model's performance and the accuracy of the classification. Additionally, be mindful of the assumptions you make when dealing with missing data, as they can significantly impact the model's behavior and results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de3eb5-52d1-46b8-86fa-ec5773e6fd42",
   "metadata": {},
   "source": [
    "#Q4.\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is one of the variants of the Naive Bayes algorithm, and it's specifically designed for data with continuous features that can be modeled using a Gaussian (normal) distribution. While it is often used for binary classification problems, it can also be extended to handle multi-class classification tasks.\n",
    "\n",
    "In multi-class classification, the goal is to classify instances into one of several classes or categories. Gaussian Naive Bayes can be adapted to multi-class classification using various strategies:\n",
    "\n",
    "    One-vs-All (One-vs-Rest): In this approach, a separate binary classifier is trained for each class, treating one class as the positive class and the other classes as the negative class. When making predictions, the class with the highest probability or score from its respective binary classifier is assigned as the predicted class.\n",
    "\n",
    "    Softmax Regression: You can also extend Gaussian Naive Bayes to multi-class classification using a softmax regression (also known as multinomial logistic regression). This approach directly models the joint probability distribution of all classes and uses a softmax function to convert the output into class probabilities. It's a more general approach to multi-class classification that can handle multiple classes without the need for one-vs-all classifiers.\n",
    "\n",
    "The choice between these approaches depends on the specific problem and the nature of the data. If you have a binary Gaussian Naive Bayes classifier and want to extend it to multi-class classification, the one-vs-all approach is a common and straightforward way to do so. However, if you are starting from scratch and have multi-class data, using softmax regression might be a more natural choice.\n",
    "\n",
    "Keep in mind that the choice of the approach may also depend on the characteristics of your data and the computational resources available. Both approaches can be used effectively for multi-class classification tasks with Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2eea08-74a0-47dc-87b6-ef5034ea382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "data = pd.read_csv(\"spambase.data\")\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_scores = cross_val_score(bernoulli_nb, X, y, cv=10)\n",
    "bernoulli_accuracy = bernoulli_scores.mean()\n",
    "bernoulli_precision = precision_score(y, bernoulli_nb.fit(X, y).predict(X))\n",
    "bernoulli_recall = recall_score(y, bernoulli_nb.fit(X, y).predict(X))\n",
    "bernoulli_f1 = f1_score(y, bernoulli_nb.fit(X, y).predict(X))\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_scores = cross_val_score(multinomial_nb, X, y, cv=10)\n",
    "multinomial_accuracy = multinomial_scores.mean()\n",
    "multinomial_precision = precision_score(y, multinomial_nb.fit(X, y).predict(X))\n",
    "multinomial_recall = recall_score(y, multinomial_nb.fit(X, y).predict(X))\n",
    "multinomial_f1 = f1_score(y, multinomial_nb.fit(X, y).predict(X))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gaussian_nb = GaussianNB()\n",
    "gaussian_scores = cross_val_score(gaussian_nb, X, y, cv=10)\n",
    "gaussian_accuracy = gaussian_scores.mean()\n",
    "gaussian_precision = precision_score(y, gaussian_nb.fit(X, y).predict(X))\n",
    "gaussian_recall = recall_score(y, gaussian_nb.fit(X, y).predict(X))\n",
    "gaussian_f1 = f1_score(y, gaussian_nb.fit(X, y).predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
